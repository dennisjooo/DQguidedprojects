---
title: "Building a Spam Filter using Naive Bayes in R"
author: "Dennis Jonathan"
date: "6/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
We're going to study the practical side of the algorithm by building a spam filter for SMS messages.

To classify messages as spam or non-spam, we saw in the previous mission that the computer:

-  Learns how humans classify messages.
- Uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.
- Classifies a new message based on these probability values — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may need a human to classify the message).

So our first task is to "teach" the computer how to classify messages. To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.

The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the The UCI Machine Learning Repository. You can also download the dataset directly from [this link](https://dq-content.s3.amazonaws.com/433/SMSSpamCollection). The data collection process is described in more details on [this page](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition), where you can also find some of the authors' papers. The dataset in the form of a CSV file can also be downloaded [here](https://dq-content.s3.amazonaws.com/475/spam.csv).

```{r prereqs}
# Importing the libraries necessary
library(tidyverse)

# Shutting off warning messages
options(warn=-1)

# Importing the data
spam <- read_csv('spam.csv')
```
## Data Exploration
```{r de}
# Previewing the first 5 rows
head(spam,5)

# Finding the column types
glimpse(spam)

# Looking for mising values
colSums(is.na(spam))
```
So we know that our dataset contains 1,000 rows and 2 columns. Those columns are:

*  `label`   : Whether the SMS is actually a **spam** or a **ham** (the opposite of a spam message)
*  `sms`     : The actual SMS message

Now we will find out how many **ham** are contained in the `label` column.

```{r de2}
# Finding the percentage of ham
spam %>% group_by(label) %>% summarize(
  Amount = n(),
  Percentage = n()/nrow(spam)
)
```
## Creating Train and Test Set
We're now going to split our dataset into a training and a test set, where the training set accounts for 80% of the data, and the test set for the remaining 20%.
```{r traintest}
# Setting random seed
set.seed(1)

# Calculate some helper values to split the dataset
n <- nrow(spam)
n_training <- 0.8 * n
n_cv <- 0.1 * n
n_test <- 0.1 * n

# Create the random indices for training set
train_indices <- sample(1:n, size = n_training, replace = FALSE)

# Get indices not used by the training set
remaining_indices <- setdiff(1:n, train_indices)

# Remaining indices are already randomized, just allocate correctly
cv_indices <- remaining_indices[1:(length(remaining_indices)/2)]
test_indices <- remaining_indices[((length(remaining_indices)/2) + 1):length(remaining_indices)]

# Use the indices to create each of the datasets
spam_train <- spam[train_indices,]
spam_cv <- spam[cv_indices,]
spam_test <- spam[test_indices,]

# Sanity check: are the ratios of ham to spam relatively constant?
print(mean(spam_train$label == "ham"))
print(mean(spam_cv$label == "ham"))
print(mean(spam_test$label == "ham"))
```
We can see that there are no set with that contains all **ham** and no **spam**, thus we can progress further.

## Data Cleaning
To calculate all the probabilities required by the algorithm, we'll first need to perform a bit of data cleaning to bring the data in a format that will allow us to extract easily all the information we need.
```{r}
# To lowercase, removal of punctuation, weird characters, digits
tidy_train <- spam_train %>% 
  mutate(
    sms = str_to_lower(sms) %>% 
      str_squish %>% 
      str_replace_all("[[:punct:]]", "") %>% 
      str_replace_all("[\u0094\u0092\u0096\n\t]", "") %>% # Unicode characters
      str_replace_all("[[:digit:]]", "")
  )

# Creating the vocabulary
vocabulary <- NULL
messages <- tidy_train %>%  pull(sms)

# Iterate through the messages and add to the vocabulary
for (m in messages) {
  words <- str_split(m, " ")[[1]]
  vocabulary <- c(vocabulary, words)
}
# Remove duplicates from the vocabulary 
vocabulary <- vocabulary %>% unique()
```

## Calculating Constants
We're now done with cleaning the training set, and we can begin creating the spam filter. The Naive Bayes algorithm will need to answer these two probability questions to be able to classify new messages:

\begin{equation}
P(Spam | w_1,w_2, ..., w_n) \propto P(Spam) \cdot \prod_{i=1}^{n}P(w_i|Spam)
\end{equation}

\begin{equation}
P(Ham | w_1,w_2, ..., w_n) \propto P(Ham) \cdot \prod_{i=1}^{n}P(w_i|Ham)
\end{equation}


Also, to calculate P(w<sub>i</sub>|Spam) and P(w<sub>i</sub>|Ham) inside the formulas above, we'll need to use these equations:

\begin{equation}
P(w_i|Spam) = \frac{N_{w_i|Spam} + \alpha}{N_{Spam} + \alpha \cdot N_{Vocabulary}}
\end{equation}

\begin{equation}
P(w_i|Ham) = \frac{N_{w_i|Ham} + \alpha}{N_{Ham} + \alpha \cdot N_{Vocabulary}}
\end{equation}


Some of the terms in the four equations above will have the same value for every new message. We can calculate the value of these terms once and avoid doing the computations again when a new messages comes in. Below, we'll use our training set to calculate:

- P(Spam) and P(Ham)
- N<sub>Spam</sub>, N<sub>Ham</sub>, N<sub>Vocabulary</sub>

We'll also use Laplace smoothing and set $\alpha = 1$.

```{r const}
# Pulling out spam messages
spam_mes <- tidy_train %>% filter(label == 'spam') %>% pull(sms)

# Creating a vocab of only spams
spam_voc <- NULL
for (i in spam_mes){
  words <- str_split(i, " ")[[1]]
  spam_voc <- c(spam_voc, words)
}
# Keeping the unique values of spam vocab
spam_voc <- spam_voc %>% unique()

# Counting the N of spam vocab
n_spam_voc <- length(spam_voc)

# Pulling out ham messages
ham_mes <- tidy_train %>% filter(label == 'ham') %>% pull(sms)

# Creating a vocab of only hams
ham_voc <- NULL
for (i in ham_mes){
  words <- str_split(i, " ")[[1]]
  ham_voc <- c(ham_voc, words)
}
# Keeping the unique values of spam vocab
ham_voc <- ham_voc %>% unique

# Counting the N of ham vocab
n_ham_voc <- length(ham_voc)

# Counting the N of vocabulary
n_vocab <- length(vocabulary)
```
## Calculating Probability of Parameters
We will now count how many times each word appear in both **spam** and **ham**. This number will vary on the train indices that we have chosen.
```{r prob par }
# Marginal probability of a training message being spam or ham
p_spam <- mean(tidy_train$label == "spam")
p_ham <- mean(tidy_train$label == "ham")

# Break up the spam and ham counting into their own tibbles
spam_counts <- tibble(
  word = spam_voc
) %>% 
  mutate(
    # Calculate the number of times a word appears in spam
    spam_count = map_int(word, function(w) {
      
      # Count how many times each word appears in all spam messsages, then sum
      map_int(spam_mes, function(sm) {
        (str_split(sm, " ")[[1]] == w) %>% sum # for a single message
      }) %>% 
        sum # then summing over all messages
      
    })
  )

ham_counts <- tibble(
  word = ham_voc
) %>% 
  mutate(
    # Calculate the number of times a word appears in ham
    ham_count = map_int(word, function(w) {
      
      # Count how many times each word appears in all ham messsages, then sum
      map_int(ham_mes, function(hm) {
        (str_split(hm, " ")[[1]] == w) %>% sum 
      }) %>% 
        sum
      
    })
  )
# Join these tibbles together
word_counts <- full_join(spam_counts, ham_counts, by = "word") %>% 
  mutate(
    # Fill in zeroes where there are missing values
    spam_count = ifelse(is.na(spam_count), 0, spam_count),
    ham_count = ifelse(is.na(ham_count), 0, ham_count)
  )
```
## Classifying A New Message

Now that we have all our parameters calculated, we can start creating the spam filter. The spam filter can be understood as a function that:

- Takes in as input a new message (w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>).
- Calculates P(Spam|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>) and P(Ham|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>).
- Compares the values of P(Spam|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>) and P(Ham|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>), and:
    - If P(Ham|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>) > P(Spam|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>), then the message is classified as ham.
    - If P(Ham|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>) < P(Spam|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>), then the message is classified as spam.
    -  If P(Ham|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>) = P(Spam|w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>), then the algorithm may request human help.
    
```{r filter}
# Creating the function
classify <- function(message, alpha = 1) {
  
  # Splitting and cleaning the new message
  clean_message <- str_to_lower(message) %>% 
    str_squish %>% 
      str_replace_all("[[:punct:]]", "") %>% 
      str_replace_all("[\u0094\u0092\u0096\n\t]", "") %>% # Unicode characters
      str_replace_all("[[:digit:]]", "")
  
  words <- str_split(clean_message, " ")[[1]]

  # Find the words that aren't present in the training
  new_words <- setdiff(vocabulary, words)
  
  # Add them to the word_counts 
  new_word_probs <- tibble(
    word = new_words,
    spam_prob = 1,
    ham_prob = 1
  )
  # Filter down the probabilities to the words present 
  present_probs <- word_counts %>% 
    filter(word %in% words) %>% 
    mutate(
      # Calculate the probabilities from the counts
      spam_prob = (spam_count + alpha) / (n_spam_voc + alpha * n_vocab),
      ham_prob = (ham_count + alpha) / (n_ham_voc + alpha * n_vocab)
    ) %>% 
    bind_rows(new_word_probs) %>% 
    pivot_longer(
      cols = c("spam_prob", "ham_prob"),
      names_to = "label",
      values_to = "prob"
    ) %>% 
    group_by(label) %>% 
    summarize(
      wi_prob = prod(prob) # prod is like sum, but with multiplication
    )
 
  # Calculate the conditional probabilities
  p_spam_given_message <- p_spam * (present_probs %>% filter(label == "spam_prob") %>% pull(wi_prob))
  p_ham_given_message <- p_ham * (present_probs %>% filter(label == "ham_prob") %>% pull(wi_prob))
  
  # Classify the message based on the probability
  ifelse(p_spam_given_message >= p_ham_given_message, "spam", "ham")
}

# Using the classify function to classify the messages in the training set
final_train <- tidy_train %>% 
  mutate(
    prediction = map_chr(sms, function(m) { classify(m) })
  ) 
```
## Calculating Accuracy
We will now try and measure the accuracy of the filter on our train data set.

```{r accuracy}
# Results of classification on training
confusion <- table(final_train$label, final_train$prediction)

# Calculating the accuracy
accuracy <- (confusion[1,1] + confusion[2,2]) / nrow(final_train)
```
We have an accuracy of `r (confusion[1,1] + confusion[2,2]) / nrow(final_train)` on our train set. 

## Tuning the $\alpha$ 
The accuracy might look good, but the problem is it is tested on the train set itself, we will need to test it on the test set. Also, there is a parameter that can tune, which is the $\alpha$ on the Laplace Smoothing. We will create a cross-validation function of for different $\alpha$.

```{r tuning}
# Generating different values of alpha
alpha_grid <- seq(0.05, 1, by = 0.05)

cv_accuracy <- NULL

# Iterating for different values of alpha
for (alpha in alpha_grid) {
  
  # Recalculate probabilities based on new alpha
  cv_probs <- word_counts %>% 
    mutate(
      # Calculate the probabilities from the counts based on new alpha
      spam_prob = (spam_count + alpha) / (n_spam_voc + alpha * n_vocab),
      ham_prob = (ham_count + alpha) / (n_ham_voc + alpha * n_vocab)
    )
  
  # Predict the classification of each message in cross validation
  cv <- spam_cv %>% 
    mutate(
      prediction = map_chr(sms, function(m) { classify(m, alpha = alpha) })
    ) 
  
  # Assess the accuracy of the classifier on cross-validation set
  confusion <- table(cv$label, cv$prediction)
  acc <- (confusion[1,1] + confusion[2,2]) / nrow(cv)
  cv_accuracy <- c(cv_accuracy, acc)
}
# Check out what the best alpha value is
tibble(
  alpha = alpha_grid,
  accuracy = cv_accuracy
)
```
We can see that the accuracy is affected by our choice of $\alpha$, but we also need to remember that the accuracy itself is also dependent on the sample that we are using, thus this will affect our choice. For this reason, we will choose an arbitrary $\alpha$ of $0.2$.
## Test Set Performance
The test set is practically new to the computer, it has never ever seen it (well technically it has, but never analyze it word for word). We will try and use our algorithm on our test set with the aforementioned $\alpha$.

```{r test}
# Reestablishing the proper parameters
optimal_alpha <- 0.2

# Using optimal alpha with training parameters, perform final predictions
spam_test <- spam_test %>% 
  mutate(
    prediction = map_chr(sms, function(m) { classify(m, alpha = optimal_alpha)} )
    )
  
confusion <- table(spam_test$label, spam_test$prediction)
test_accuracy <- (confusion[1,1] + confusion[2,2]) / nrow(spam_test)
test_accuracy
```
In the end, we reached an accuracy of `r test_accuracy`. 